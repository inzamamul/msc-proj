{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "\n",
    "much_data = np.load('training_np--30--30.npy')\n",
    "train_data = much_data[:3000]\n",
    "validation_data = much_data[3001:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1441, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(train_data)\n",
    "np.random.shuffle(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "sess = tf.InteractiveSession()\n",
    "#build a softmax regression model\n",
    "# shape arg is optional, lets tf automatically catch bugs from inconsistent tensorshapes\n",
    "x = tf.placeholder(tf.float32, shape=[None, 30, 30], name = \"x\") # 28x28 pixel image = 784, None indicates frst dimension\n",
    "y_ = tf.placeholder(tf.float32, shape = [None, 2], name = \"y\") #, shape = [2 ]) # , shape = [None, 2]) # one hot 10-dimensional vector indictaing which digit class (0-9)\n",
    "\n",
    "\n",
    "#\n",
    "###\n",
    "###### MULTILAYER COMPUTATIONAL GRAPH\n",
    "\n",
    "# TODO. Beautify tensorbaord by using tf.scope and wrapping names around the weight and bias variables \n",
    "\n",
    "def weight_variable(shape, w_name):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "\n",
    "    return tf.Variable(initial, name = 'NN'+str(w_name)+'/W')\n",
    "\n",
    "# add bias 0.1 to avoid 'dead neurons' from using ReLU neurons/ activation functions\n",
    "def bias_variable(shape, b_name):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial, name = 'NN'+str(b_name)+'/B')\n",
    "\n",
    "# convolution and pooling. handling the boundaries and stride sizes. This is the vanilla vesion (stride of 1, zero padding, output is same size as input)\n",
    "# pooling is plain old max pooling over 2x2 blocks\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first convolutional layer. consists of our first convolution, followed by max pooling\n",
    "# convolution will compute 32 features for each 5x5 patch\n",
    "# weight tensor wil thus have shape of [5, 5, 1, 32] i.e. [ patch size, patch size, input channels, output channels]\n",
    "with tf.variable_scope('ConvLayer1'):\n",
    "    W_conv1 = weight_variable([5, 5, 1, 32], 1)\n",
    "    b_conv1 = bias_variable([32], 1) # bias vector\n",
    "\n",
    "# reshape x to a 4d tensor, with the 2nd and 3rd dimensions corresponding to image width and height, last dimension is # of color channels\n",
    "x_image = tf.reshape(x, [-1, 30, 30, 1])\n",
    "\n",
    "# convolve x_image with the weight tesnor, add bias, apply ReLU function, then max pool\n",
    "# max pool_2x2 resizes image size to 14x14\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# second convolutional layer \n",
    "# will have 64 features for each 5x5 patch\n",
    "# with tf.variable_scope('ConvLayer2'):\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 64], 2)\n",
    "b_conv2 = bias_variable([64], 2)\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "\n",
    "# densely connected layer. Image size has been reduced to 7x7, we add a full_connected layer with 1024 neurons to allow processing on the entire image\n",
    "# reshape the tensor from the pooling layer into a batch of vectors, multiple by a weight matrix, add bias, apply ReLU\n",
    "# with tf.variable_scope('DenseConnLayer'):\n",
    "W_fc1 = weight_variable([8 * 8 * 64, 1024], 'fc1')\n",
    "b_fc1 = bias_variable([1024], 'fc1')\n",
    "\n",
    "# with tf.variable_scope('ReshapeImg'):\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 8*8*64])\n",
    "h_fc1 = tf.nn.tanh(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# add dropout to reduce overfitting - before readout layer. Dropout only really useful in large CNNs\n",
    "# turn dropout on during training, turn it off during testing\n",
    "# with tf.variable_scope('DropOut'):\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "\n",
    "# readout layer\n",
    "W_fc2 = weight_variable([1024, 2], 'fc2')\n",
    "b_fc2 = bias_variable([2], 'fc2')\n",
    "\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/inz/anaconda/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "step 0, training accuracy 0.24\n",
      "step 50, training accuracy 0.72\n",
      "step 100, training accuracy 0.82\n",
      "step 150, training accuracy 0.72\n",
      "step 200, training accuracy 0.82\n",
      "step 250, training accuracy 0.68\n",
      "step 300, training accuracy 0.74\n",
      "step 350, training accuracy 0.74\n",
      "step 400, training accuracy 0.78\n",
      "step 450, training accuracy 0.76\n",
      "step 500, training accuracy 0.86\n",
      "step 550, training accuracy 0.68\n",
      "step 600, training accuracy 0.76\n",
      "step 650, training accuracy 0.86\n",
      "step 700, training accuracy 0.76\n",
      "step 750, training accuracy 0.68\n",
      "step 800, training accuracy 0.62\n",
      "step 850, training accuracy 0.72\n",
      "step 900, training accuracy 0.74\n",
      "step 950, training accuracy 0.68\n",
      "test accuracy 0.748092\n"
     ]
    }
   ],
   "source": [
    "# train and evalutate the model\n",
    "# \n",
    "\n",
    "keep_prob1 = tf.placeholder(tf.float32)\n",
    "\n",
    "with tf.name_scope('cross_entropy'):\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels= y_, logits=y_conv))\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy) # step size is very sensitive\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "tf.summary.scalar(\"cost\", cross_entropy)\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "logs_path = \"logs/\"\n",
    "# train_writer = tf.summary.FileWriter(FLAGS.summaries_dir + '/train', sess.graph)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # run session, initialise variables\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    # writes to TensorBoard \n",
    "    writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    batch_size = 50\n",
    "    \n",
    "    for k in range(1000):\n",
    "        \n",
    "        if k % batch_size == 0 :\n",
    "    \n",
    "            d = train_data[k: k+batch_size]\n",
    "            X, Y = zip(*d)\n",
    "            X = np.stack(list(X), axis=0)\n",
    "            Y = np.vstack(list(Y))\n",
    "            X = X.astype(float)\n",
    "            Y = Y.astype(float)\n",
    "            a = np.transpose(Y)\n",
    "            c = list()\n",
    "#             c = np.array()\n",
    "\n",
    "            for i in Y:\n",
    "                # t=0\n",
    "                if i == 1:\n",
    "                    c.append(np.array([0.0,1.0]))\n",
    "                # t=23 \n",
    "                if i == 0:\n",
    "                    c.append(np.array([1.0,0.0]))\n",
    "\n",
    "            Y = np.vstack(c)\n",
    "            \n",
    "            train_accuracy = accuracy.eval(feed_dict={x: X, y_: Y, keep_prob: 1.0})\n",
    "            \n",
    "            print(\"step %d, training accuracy %g\" % (k, train_accuracy))\n",
    "            \n",
    "        optimizer.run(feed_dict={x : X, y_ : Y, keep_prob:0.9})\n",
    "\n",
    "    # Creating the Validation Data to validate the model with (and see if it its overfitting)\n",
    "    d = validation_data\n",
    "    X, Y = zip(*d)\n",
    "    X = np.stack(list(X), axis=0)\n",
    "    Y = np.vstack(list(Y))\n",
    "    X = X.astype(float)\n",
    "    Y = Y.astype(float)\n",
    "    a = np.transpose(Y)\n",
    "    c = list()\n",
    "#     c = np.array()\n",
    "    \n",
    "    for i in Y: \n",
    "        if i == 1:\n",
    "            c.append(np.array([0.0,1.0]))\n",
    "        if i == 0:\n",
    "            c.append(np.array([1.0,0.0]))\n",
    "\n",
    "    Y = np.vstack(c)\n",
    "    \n",
    "\n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict={x: X, y_: Y, keep_prob: 1.0 }))\n",
    "    \n",
    "    prediction=sess.run(y_conv ,feed_dict={x: X, keep_prob: 1.0})\n",
    "#     preds = prediction.eval()\n",
    "    \n",
    "#     for i in range(len(preds)):\n",
    "#         print preds[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 128.  129.  129.  128.  128.  128.  128.  128.  128.  130.  127.  125.\n",
      "   126.  129.  136.  144.  151.  144.  134.  133.  142.  149.  164.  168.\n",
      "   159.  136.  129.  126.  126.  127.]\n",
      " [ 127.  129.  129.  128.  128.  128.  126.  127.  129.  130.  131.  134.\n",
      "   137.  135.  142.  154.  161.  148.  131.  128.  138.  151.  161.  167.\n",
      "   160.  140.  133.  129.  126.  127.]\n",
      " [ 126.  128.  129.  129.  129.  127.  124.  124.  130.  132.  138.  148.\n",
      "   154.  146.  153.  168.  175.  153.  126.  118.  128.  151.  157.  165.\n",
      "   161.  143.  139.  133.  128.  126.]\n",
      " [ 127.  127.  127.  127.  130.  128.  127.  125.  129.  135.  143.  151.\n",
      "   156.  151.  150.  157.  159.  137.  111.   99.  105.  127.  160.  171.\n",
      "   159.  137.  138.  139.  135.  126.]\n",
      " [ 128.  127.  126.  126.  130.  130.  130.  128.  128.  139.  142.  141.\n",
      "   142.  146.  142.  140.  137.  121.  100.   82.   78.  101.  158.  167.\n",
      "   143.  122.  136.  146.  141.  127.]\n",
      " [ 129.  128.  128.  128.  129.  129.  130.  131.  134.  139.  122.  100.\n",
      "    95.  122.  137.  142.  139.  128.  114.   80.   59.   94.  140.  128.\n",
      "    95.   90.  132.  152.  146.  128.]\n",
      " [ 129.  129.  129.  129.  129.  129.  129.  132.  139.  141.  111.   78.\n",
      "    73.   99.  122.  133.  136.  130.  122.   77.   45.   89.  108.   88.\n",
      "    67.   79.  135.  156.  147.  128.]\n",
      " [ 128.  127.  126.  125.  128.  130.  129.  132.  143.  146.  116.   93.\n",
      "    97.   84.   91.  107.  118.  116.  112.   66.   34.   83.   56.   55.\n",
      "    78.  107.  148.  156.  143.  128.]\n",
      " [ 128.  127.  127.  127.  128.  129.  128.  130.  142.  156.  141.  122.\n",
      "   115.   88.   73.   76.   91.  106.   92.   44.   19.   73.   56.   67.\n",
      "   100.  136.  165.  158.  138.  126.]\n",
      " [ 127.  128.  130.  131.  129.  127.  125.  127.  138.  169.  178.  160.\n",
      "   125.  110.   73.   51.   61.  102.   68.   19.    7.   66.  102.  115.\n",
      "   130.  164.  182.  160.  133.  123.]\n",
      " [ 128.  129.  130.  133.  130.  127.  126.  130.  142.  165.  174.  154.\n",
      "   112.  118.  112.  100.   96.  113.   74.   39.   42.   98.  132.  142.\n",
      "   150.  170.  174.  156.  135.  125.]\n",
      " [ 130.  129.  130.  132.  130.  128.  130.  137.  149.  154.  154.  133.\n",
      "    99.  127.  163.  173.  152.  130.   91.   71.   89.  142.  153.  155.\n",
      "   158.  163.  155.  149.  140.  129.]\n",
      " [ 128.  127.  128.  130.  129.  129.  136.  146.  151.  144.  143.  137.\n",
      "   129.  162.  194.  203.  180.  142.   88.   65.   92.  159.  166.  152.\n",
      "   142.  149.  143.  146.  145.  138.]\n",
      " [ 127.  126.  127.  129.  129.  132.  142.  151.  149.  137.  133.  136.\n",
      "   146.  175.  192.  197.  184.  153.  101.   75.   94.  153.  148.  128.\n",
      "   119.  133.  135.  144.  148.  144.]\n",
      " [ 128.  128.  128.  127.  130.  138.  146.  149.  141.  134.  123.  117.\n",
      "   124.  141.  143.  146.  155.  164.  149.  126.  112.  118.   81.   77.\n",
      "    97.  115.  128.  142.  148.  145.]\n",
      " [ 128.  129.  128.  126.  129.  141.  148.  146.  140.  131.  119.  111.\n",
      "   111.  100.   87.   97.  133.  181.  185.  165.  131.   95.   40.   40.\n",
      "    74.  108.  124.  142.  149.  144.]\n",
      " [ 130.  129.  127.  126.  126.  142.  148.  146.  143.  127.  120.  119.\n",
      "   109.   57.   32.   55.  122.  201.  206.  186.  148.   85.   30.   21.\n",
      "    55.  111.  122.  144.  152.  141.]\n",
      " [ 131.  130.  128.  128.  129.  144.  148.  142.  134.  118.  109.  105.\n",
      "    96.   48.   26.   52.  121.  207.  220.  185.  125.   65.   27.   32.\n",
      "    68.  112.  126.  149.  153.  138.]\n",
      " [ 132.  130.  129.  131.  135.  146.  147.  137.  121.  107.   90.   82.\n",
      "    80.   55.   45.   70.  128.  198.  212.  163.   89.   47.   34.   61.\n",
      "    98.  118.  137.  155.  153.  135.]\n",
      " [ 130.  129.  128.  130.  134.  144.  147.  137.  116.  100.   74.   62.\n",
      "    71.   46.   39.   78.  141.  157.  142.  115.   83.   54.   59.   87.\n",
      "   120.  143.  160.  161.  149.  132.]\n",
      " [ 123.  125.  126.  128.  132.  141.  147.  141.  120.  101.   71.   56.\n",
      "    67.   40.   38.   89.  151.  115.   73.   72.   90.   76.   88.  110.\n",
      "   140.  172.  180.  164.  143.  130.]\n",
      " [ 110.  117.  123.  125.  129.  139.  151.  153.  138.  115.   93.   77.\n",
      "    68.   40.   58.  111.  150.   85.   37.   57.  106.  112.  114.  126.\n",
      "   158.  200.  188.  157.  134.  129.]\n",
      " [ 104.  112.  120.  124.  128.  136.  149.  158.  154.  134.  116.  100.\n",
      "    85.   64.   91.  135.  152.   88.   44.   71.  129.  146.  137.  141.\n",
      "   167.  202.  179.  147.  128.  129.]\n",
      " [ 108.  111.  118.  125.  129.  132.  141.  155.  166.  157.  138.  123.\n",
      "   120.  113.  138.  160.  159.  128.   98.  115.  157.  175.  157.  154.\n",
      "   165.  172.  154.  133.  124.  128.]\n",
      " [ 117.  117.  121.  128.  130.  129.  134.  146.  162.  168.  159.  148.\n",
      "   142.  139.  157.  171.  169.  155.  134.  143.  170.  188.  171.  159.\n",
      "   155.  149.  138.  129.  126.  129.]\n",
      " [ 128.  127.  128.  131.  132.  127.  127.  135.  149.  167.  176.  170.\n",
      "   154.  149.  159.  171.  177.  171.  157.  157.  170.  185.  177.  157.\n",
      "   139.  131.  130.  131.  132.  130.]\n",
      " [ 133.  130.  129.  131.  133.  129.  126.  128.  139.  150.  163.  168.\n",
      "   162.  160.  160.  162.  166.  165.  160.  157.  157.  161.  157.  142.\n",
      "   128.  122.  126.  130.  132.  129.]\n",
      " [ 133.  130.  129.  130.  132.  131.  127.  126.  132.  134.  144.  155.\n",
      "   160.  162.  153.  148.  150.  152.  152.  148.  141.  136.  134.  129.\n",
      "   123.  120.  125.  129.  130.  128.]\n",
      " [ 130.  129.  130.  131.  129.  128.  128.  128.  130.  130.  132.  136.\n",
      "   139.  141.  134.  131.  135.  139.  138.  135.  130.  127.  128.  129.\n",
      "   128.  123.  125.  126.  126.  127.]\n",
      " [ 128.  129.  131.  131.  127.  126.  128.  130.  129.  130.  126.  124.\n",
      "   125.  127.  121.  121.  127.  131.  129.  127.  125.  123.  125.  130.\n",
      "   132.  125.  125.  125.  125.  126.]]\n"
     ]
    }
   ],
   "source": [
    "print X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-248e465fb823>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;31m# run session, initialise variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'v' is not defined"
     ]
    }
   ],
   "source": [
    "# train and evalutate the model\n",
    "# \n",
    "\n",
    "keep_prob1 = tf.placeholder(tf.float32)\n",
    "\n",
    "with tf.name_scope('cross_entropy'):\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels= y_, logits=y_conv))\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy) # step size is very sensitive\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "tf.scalar_summary(\"cost\", cross_entropy)\n",
    "tf.scalar_summary(\"accuracy\", accuracy)\n",
    "\n",
    "summary_op = tf.merge_all_summaries()\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "epoch_loss = 0 \n",
    "\n",
    "logs_path = \"logs/\"\n",
    "# train_writer = tf.summary.FileWriter(FLAGS.summaries_dir + '/train', sess.graph)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    v\n",
    "    # run session, initialise variables\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    # writes to TensorBoard \n",
    "    writer = tf.train.SummaryWriter(logs_path, graph=tf.get_default_graph())\n",
    "  \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        batch_count = 50\n",
    "    \n",
    "    #   for i in range(0, len(train_data), 20):\n",
    "        for i in range(0, 2001, batch_count):\n",
    "    \n",
    "            d = train_data[i: i+50]\n",
    "            X, Y = zip(*d)\n",
    "            X = np.stack(list(X), axis=0)\n",
    "            Y = np.vstack(list(Y))\n",
    "            X = X.astype(float)\n",
    "            Y = Y.astype(float)\n",
    "            a = np.transpose(Y)\n",
    "            c = list()\n",
    "\n",
    "            for i in Y:\n",
    "                # t=0\n",
    "                if i == 1:\n",
    "                    c.append(np.array([0.0,1.0]))\n",
    "                # t=23 \n",
    "                if i == 0:\n",
    "                    c.append(np.array([1.0,0.0]))\n",
    "\n",
    "            Y = np.vstack(c)\n",
    "\n",
    "            loss_ = sess.run(cross_entropy, feed_dict={x: X , y_ : Y, keep_prob: 1.0})\n",
    "            accur_ = accuracy.eval(feed_dict={x: X, y_: Y, keep_prob: 1.0})\n",
    "\n",
    "            print(\"Loss: \", loss_ , \"::: Train acc:\", accur_  )\n",
    "            \n",
    "            _, c = sess.run([optimizer, cross_entropy], feed_dict={x:X, y_:Y, keep_prob: 1.0})\n",
    "\n",
    "#BIG TODO            \n",
    "#             _, summary = sess.run([optimizer, summary_op], feed_dict={x:X, y_:Y keep_prob: 0.9})\n",
    "            \n",
    "#             writer.add_summary(summary, epoch * batch_count + i)\n",
    "\n",
    "    print('Epoch', epoch, 'completed out of',epochs,'loss:', loss_)\n",
    "\n",
    "    # Creating the Validation Data to validate the model with (and see if it its overfitting)\n",
    "    d = validation_data\n",
    "    X, Y = zip(*d)\n",
    "    X = np.stack(list(X), axis=0)\n",
    "    Y = np.vstack(list(Y))\n",
    "    X = X.astype(float)\n",
    "    Y = Y.astype(float)\n",
    "    a = np.transpose(Y)\n",
    "    c = list()\n",
    "    \n",
    "    for i in Y: \n",
    "        if i == 1:\n",
    "            c.append(np.array([0.0,1.0]))\n",
    "        if i == 0:\n",
    "            c.append(np.array([1.0,0.0]))\n",
    "\n",
    "    Y = np.vstack(c)\n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict={x: X, y_: Y, keep_prob: 1.0 }))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
